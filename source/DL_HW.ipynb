{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSkgt1zf-raF",
        "outputId": "c2e0f1bd-d2a8-4811-ad5f-0c03aae87aa4",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch_geometric in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch_geometric) (3.11.18)\n",
            "Requirement already satisfied: fsspec in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch_geometric) (2.2.5)\n",
            "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch_geometric) (7.0.0)\n",
            "Requirement already satisfied: pyparsing in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->torch_geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->torch_geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->torch_geometric) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.0 in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->torch_geometric) (3.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torch_geometric) (2025.4.26)\n",
            "Requirement already satisfied: colorama in c:\\users\\fede6\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->torch_geometric) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\Users\\fede6\\Desktop\\DeepHW\n"
          ]
        }
      ],
      "source": [
        "%cd Desktop/DeepHW/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lAQuCuIoBbq5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "# Load utility functions from cloned repository\n",
        "from loadData import GraphDataset\n",
        "\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, ResGatedGraphConv\n",
        "\n",
        "# from src.utils import set_seed\n",
        "# from src.models import GNN\n",
        "import argparse\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import tarfile\n",
        "\n",
        "def set_seed(seed=777):\n",
        "    seed = seed\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "def gzip_folder(folder_path, output_file):\n",
        "    \"\"\"\n",
        "    Compresses an entire folder into a single .tar.gz file.\n",
        "    \n",
        "    Args:\n",
        "        folder_path (str): Path to the folder to compress.\n",
        "        output_file (str): Path to the output .tar.gz file.\n",
        "    \"\"\"\n",
        "    with tarfile.open(output_file, \"w:gz\") as tar:\n",
        "        tar.add(folder_path, arcname=os.path.basename(folder_path))\n",
        "    print(f\"Folder '{folder_path}' has been compressed into '{output_file}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Dyf0I2-t9IcW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def add_zeros(data):\n",
        "    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3jKvoQYI9Zbc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train(data_loader, model, optimizer, criterion, device, save_checkpoints, checkpoint_path, current_epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in tqdm(data_loader, desc=\"Iterating training graphs\", unit=\"batch\"):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += (pred == data.y).sum().item()\n",
        "        total += data.y.size(0)\n",
        "\n",
        "    # Save checkpoints if required\n",
        "    # if save_checkpoints:\n",
        "    #     checkpoint_file = f\"{checkpoint_path}_epoch_{current_epoch + 1}.pth\"\n",
        "    #     torch.save(model.state_dict(), checkpoint_file)\n",
        "    #     print(f\"Checkpoint saved at {checkpoint_file}\")\n",
        "\n",
        "    return total_loss / len(data_loader),  correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8peFiIS19ZpK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def evaluate(data_loader, model, device, calculate_accuracy=False):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    ground_truths = []\n",
        "    predictions = []\n",
        "    \n",
        "    total_loss = 0\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(data_loader, desc=\"Iterating eval graphs\", unit=\"batch\"):\n",
        "            data = data.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1)\n",
        "\n",
        "            if calculate_accuracy:\n",
        "                correct += (pred == data.y).sum().item()\n",
        "                total += data.y.size(0)\n",
        "                total_loss += criterion(output, data.y).item()\n",
        "                \n",
        "                predictions.extend(pred.cpu().numpy())\n",
        "                ground_truths.extend(data.y.cpu().numpy())\n",
        "                \n",
        "                f1 = f1_score(ground_truths, predictions, average='macro')                \n",
        "                \n",
        "            else:\n",
        "                predictions.extend(pred.cpu().numpy())\n",
        "    if calculate_accuracy:\n",
        "        accuracy = correct / total\n",
        "        return  total_loss / len(data_loader), accuracy, f1\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WanuZKxy9Zs-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def save_predictions(predictions, test_path):\n",
        "    script_dir = os.getcwd()\n",
        "    submission_folder = os.path.join(script_dir, \"submission\")\n",
        "    test_dir_name = os.path.basename(os.path.dirname(test_path))\n",
        "\n",
        "    os.makedirs(submission_folder, exist_ok=True)\n",
        "\n",
        "    output_csv_path = os.path.join(submission_folder, f\"testset_{test_dir_name}.csv\")\n",
        "\n",
        "    test_graph_ids = list(range(len(predictions)))\n",
        "    output_df = pd.DataFrame({\n",
        "        \"id\": test_graph_ids,\n",
        "        \"pred\": predictions\n",
        "    })\n",
        "\n",
        "    output_df.to_csv(output_csv_path, index=False)\n",
        "    print(f\"Predictions saved to {output_csv_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uyHIJS5U9ZzB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def plot_training_progress(train_losses, train_accuracies, output_dir, f1_score=None):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    if f1_score is None:\n",
        "        # Plot loss\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training Loss per Epoch')\n",
        "\n",
        "        # Plot accuracy\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title('Training Accuracy per Epoch')\n",
        "    else:\n",
        "        # Plot loss\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(epochs, train_losses, label=\"Training Loss\", color='blue')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training Loss per Epoch')\n",
        "\n",
        "        # Plot accuracy\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(epochs, train_accuracies, label=\"Training Accuracy\", color='green')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title('Training Accuracy per Epoch')\n",
        "        \n",
        "        # Plot f1-score\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.plot(epochs, f1_score, label=\"Training F1-score\", color='red')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('F1-score')\n",
        "        plt.title('Training F1-score per Epoch')\n",
        "\n",
        "    # Save plots in the current directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, \"training_progress.png\"))\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8ZfF5Ypw0BHU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_user_input(prompt, default=None, required=False, type_cast=str):\n",
        "\n",
        "    while True:\n",
        "        user_input = input(f\"{prompt} [{default}]: \")\n",
        "\n",
        "        if user_input == \"\" and required:\n",
        "            print(\"This field is required. Please enter a value.\")\n",
        "            continue\n",
        "\n",
        "        if user_input == \"\" and default is not None:\n",
        "            return default\n",
        "\n",
        "        if user_input == \"\" and not required:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            return type_cast(user_input)\n",
        "        except ValueError:\n",
        "            print(f\"Invalid input. Please enter a valid {type_cast.__name__}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SbkHNNsr0BHV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_arguments():\n",
        "    args = {}\n",
        "    args['train_path'] = get_user_input(\"Path to the training dataset (optional)\")\n",
        "    args['test_path'] = get_user_input(\"Path to the test dataset\", required=True)\n",
        "    args['num_checkpoints'] = get_user_input(\"Number of checkpoints to save during training\", type_cast=int)\n",
        "    args['device'] = get_user_input(\"Which GPU to use if any\", default=1, type_cast=int)\n",
        "    args['gnn'] = get_user_input(\"GNN type (gin, gin-virtual, gcn, gcn-virtual)\", default='gin')\n",
        "    args['drop_ratio'] = get_user_input(\"Dropout ratio\", default=0.0, type_cast=float)\n",
        "    args['num_layer'] = get_user_input(\"Number of GNN message passing layers\", default=5, type_cast=int)\n",
        "    args['emb_dim'] = get_user_input(\"Dimensionality of hidden units in GNNs\", default=300, type_cast=int)\n",
        "    args['batch_size'] = get_user_input(\"Input batch size for training\", default=32, type_cast=int)\n",
        "    args['epochs'] = get_user_input(\"Number of epochs to train\", default=10, type_cast=int)\n",
        "    args['baseline_mode'] = get_user_input(\"Baseline mode: 1 (CE), 2 (Noisy CE)\", default=1, type_cast=int)\n",
        "    args['noise_prob'] = get_user_input(\"Noise probability p (used if baseline_mode=2)\", default=0.2, type_cast=float)\n",
        "\n",
        "\n",
        "    return argparse.Namespace(**args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nozPrW9C0BHW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class NoisyCrossEntropyLoss(torch.nn.Module):\n",
        "    def __init__(self, p_noisy):\n",
        "        super().__init__()\n",
        "        self.p = p_noisy\n",
        "        self.ce = torch.nn.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        losses = self.ce(logits, targets)\n",
        "        weights = (1 - self.p) + self.p * (1 - torch.nn.functional.one_hot(targets, num_classes=logits.size(1)).float().sum(dim=1))\n",
        "        return (losses * weights).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arguments received:\n",
            "train_path: C:\\Users\\fede6\\Desktop\\DeepHW\\data\\A\\train.json.gz\n",
            "test_path: C:\\Users\\fede6\\Desktop\\DeepHW\\data\\A\\test.json.gz\n",
            "num_checkpoints: 5\n",
            "device: 1\n",
            "gnn: gin-virtual\n",
            "drop_ratio: 0.5\n",
            "num_layer: 5\n",
            "emb_dim: 300\n",
            "batch_size: 32\n",
            "epochs: 50\n",
            "baseline_mode: 2\n",
            "noise_prob: 0.5\n"
          ]
        }
      ],
      "source": [
        "def populate_args(args):\n",
        "    print(\"Arguments received:\")\n",
        "    for key, value in vars(args).items():\n",
        "        print(f\"{key}: {value}\")\n",
        "args = get_arguments()\n",
        "populate_args(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Arguments received:\n",
        "train_path: C:\\Users\\fede6\\Desktop\\DeepHW\\data\\A\\train.json.gz\n",
        "test_path: C:\\Users\\fede6\\Desktop\\DeepHW\\data\\A\\test.json.gz\n",
        "num_checkpoints: 5\n",
        "device: 1\n",
        "gnn: gin\n",
        "drop_ratio: 0.5\n",
        "num_layer: 5\n",
        "emb_dim: 300\n",
        "batch_size: 32\n",
        "epochs: 50\n",
        "baseline_mode: 2\n",
        "noise_prob: 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch_geometric.utils import dropout_edge\n",
        "\n",
        "\n",
        "def DE_addZero(data, drop_prob=0.2):\n",
        "    # DropEdge\n",
        "    edge_index, edge_attr = dropout_edge(\n",
        "        data.edge_index,\n",
        "        data.edge_attr,\n",
        "        p=0.2,\n",
        "        force_undirected=False,\n",
        "        num_nodes=data.num_nodes,\n",
        "        training=True\n",
        "    )\n",
        "    data.edge_index = edge_index\n",
        "    data.edge_attr = edge_attr\n",
        "\n",
        "    # Add zero node features\n",
        "    data.x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
        "\n",
        "    return data\n",
        "\n",
        "def normalize_edge_attr(data):\n",
        "    if data.edge_attr is not None:\n",
        "        data.edge_attr = (data.edge_attr - data.edge_attr.mean(dim=0)) / (data.edge_attr.std(dim=0) + 1e-6)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading graphs from C:\\Users\\fede6\\Desktop\\DeepHW\\data\\A\\train.json.gz...\n",
            "This may take a few minutes, please wait...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing graphs: 100%|██████████████████████████████████████████| 11280/11280 [00:35<00:00, 318.32graph/s]\n"
          ]
        }
      ],
      "source": [
        "full_dataset = GraphDataset(args.train_path, transform=add_zeros, pre_transform=None)\n",
        "val_size = int(0.2 * len(full_dataset))\n",
        "train_size = len(full_dataset) - val_size\n",
        "\n",
        "generator = torch.Generator().manual_seed(12)\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, classes=6, smoothing=0.1, dim=-1):\n",
        "        \"\"\"\n",
        "        classes: numero di classi\n",
        "        smoothing: epsilon per la label smoothing\n",
        "        dim: asse su cui calcolare softmax\n",
        "        \"\"\"\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.cls = classes\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        \"\"\"\n",
        "        pred: logits (non softmaxati), shape [batch_size, num_classes]\n",
        "        target: etichette intere (non one-hot), shape [batch_size]\n",
        "        \"\"\"\n",
        "        pred = pred.log_softmax(dim=self.dim)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\fede6\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import kaggle_models as km\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "script_dir = os.getcwd()\n",
        "# device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_checkpoints = args.num_checkpoints if args.num_checkpoints else 3\n",
        "\n",
        "if args.gnn == 'gin':\n",
        "    model = km.GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False, graph_pooling='mean').to(device)\n",
        "elif args.gnn == 'gin-virtual':\n",
        "    model = km.GNN(gnn_type='gin', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True, graph_pooling='mean').to(device)\n",
        "elif args.gnn == 'gcn':\n",
        "    model = km.GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=False, graph_pooling='mean').to(device)\n",
        "elif args.gnn == 'gcn-virtual':\n",
        "    model = km.GNN(gnn_type='gcn', num_class=6, num_layer=args.num_layer, emb_dim=args.emb_dim, drop_ratio=args.drop_ratio, virtual_node=True).to(device)\n",
        "else:\n",
        "    raise ValueError('Invalid GNN type')\n",
        "\n",
        "model = km.GNN(gnn_type='gin', num_class=6, num_layer=5, emb_dim=150, drop_ratio=0.5, virtual_node=True, residual=True, graph_pooling='attention').to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-10)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='max',   # Monitor validation \n",
        "            factor=0.7,   # Reduce LR by 50% on plateau\n",
        "            patience=3,  # Number of epochs with no improvement\n",
        "            min_lr=1e-6,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "if args.baseline_mode == 2:\n",
        "    criterion = NoisyCrossEntropyLoss(args.noise_prob)\n",
        "else:\n",
        "    criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "BTYT5jYuChPb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "test_dir_name = os.path.basename(os.path.dirname(args.test_path))\n",
        "logs_folder = os.path.join(script_dir, \"logs\", test_dir_name)\n",
        "log_file = os.path.join(logs_folder, \"training.log\")\n",
        "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
        "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
        "# logging.getLogger().addHandler(logging.StreamHandler())\n",
        "\n",
        "checkpoint_path = os.path.join(script_dir, \"checkpoints\", f\"model_{test_dir_name}_best.pth\")\n",
        "checkpoints_folder = os.path.join(script_dir, \"checkpoints\", test_dir_name)\n",
        "os.makedirs(checkpoints_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "0IYjIhWV0BHX",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded best model from C:\\Users\\fede6\\Desktop\\DeepHW\\checkpoints\\model_A_best.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\fede6\\AppData\\Local\\Temp\\ipykernel_13364\\3899982052.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path)\n"
          ]
        }
      ],
      "source": [
        "restart_epoch = 63\n",
        "best_f1_score = 0.0\n",
        "checkpoint_fn = os.path.join(script_dir, \"checkpoints\", f\"A\\model_A_epoch_{restart_epoch}.pth\")\n",
        "start = 0\n",
        "if os.path.exists(checkpoint_path): # and not args.train_path:\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    restart_epoch = checkpoint['epoch']\n",
        "    best_f1_score = checkpoint['best_f1_score']\n",
        "    print(f\"Loaded best model from {checkpoint_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2587"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wLyxmJ00BHX",
        "outputId": "fe838c0e-1aa4-4b5f-c702-fe564b5a65ba",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iterating training graphs: 100%|███████████████████████████████████████| 282/282 [00:39<00:00,  7.09batch/s]\n",
            "Iterating eval graphs: 100%|█████████████████████████████████████████████| 71/71 [00:05<00:00, 13.46batch/s]\n"
          ]
        },
        {
          "ename": "NotImplementedError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m val_loss, val_acc, val_f1 = evaluate(val_loader, model, device, calculate_accuracy=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     27\u001b[39m scheduler.step(val_f1)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m actual_lr = \u001b[43mscheduler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, lr = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_lr[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[32m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:178\u001b[39m, in \u001b[36mLRScheduler.get_lr\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_lr\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    177\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute learning rate using chainable form of the scheduler.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
            "\u001b[31mNotImplementedError\u001b[39m: "
          ]
        }
      ],
      "source": [
        "if args.train_path:\n",
        "    num_epochs = args.epochs\n",
        "    best_val_accuracy = 0.0 # max(checkpoint['val_accuracy'])\n",
        "    best_f1_score =     checkpoint['best_f1_score'] # max(checkpoint['val_f1_score'])\n",
        "\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    val_f1_scores = []\n",
        "\n",
        "    if num_checkpoints > 1:\n",
        "        checkpoint_intervals = [int((i + 1) * num_epochs / num_checkpoints) for i in range(num_checkpoints)]\n",
        "    else:\n",
        "        checkpoint_intervals = [num_epochs]\n",
        "\n",
        "    for epoch in range(restart_epoch, 100):\n",
        "        train_loss, train_acc = train(\n",
        "            train_loader, model, optimizer, criterion, device,\n",
        "            save_checkpoints=(epoch + 1 in checkpoint_intervals),\n",
        "            checkpoint_path=os.path.join(checkpoints_folder, f\"model_{test_dir_name}\"),\n",
        "            current_epoch=epoch\n",
        "        )\n",
        "\n",
        "        val_loss, val_acc, val_f1 = evaluate(val_loader, model, device, calculate_accuracy=True)\n",
        "        \n",
        "        scheduler.step(val_f1)\n",
        "        \n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
        "        logging.info(f\"Epoch {epoch + 1}/{100}, Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "        val_f1_scores.append(val_f1)\n",
        "        \n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'best_f1_score': best_f1_score,\n",
        "            'train_loss': train_losses,\n",
        "            'train_accuracy': train_accuracies,\n",
        "            'val_loss': val_losses,\n",
        "            'val_accuracy': val_accuracies,\n",
        "            'val_f1_score': val_f1_scores\n",
        "        }\n",
        "        \n",
        "        if (epoch + 1)%5 == 0:\n",
        "            torch.save(checkpoint, os.path.join(script_dir, \"checkpoints\", f\"A\\model_A_epoch_{epoch+1}.pth\"))\n",
        "            print(f\"Model saved at epoch {epoch+1}\")\n",
        "        \n",
        "        if val_f1 > best_f1_score:\n",
        "            best_f1_score = val_f1\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            print(f\"Best model updated and saved at {checkpoint_path}\")\n",
        "            \n",
        "        plot_training_progress(train_losses, train_accuracies, os.path.join(logs_folder, \"plots\"))\n",
        "        plot_training_progress(val_losses, val_accuracies, os.path.join(logs_folder, \"plotsVal\"), val_f1_scores)\n",
        "        \n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_975kFY10BHX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "del train_dataset\n",
        "del train_loader\n",
        "del full_dataset\n",
        "del val_dataset\n",
        "del val_loader\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsXZIj4Mdu3I",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "test_dataset = GraphDataset(r\"C:\\Users\\fede6\\Desktop\\DeepHW\\data\\A\\test.json.gz\", transform=add_zeros)\n",
        "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1OnGq_nCmTr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(checkpoint_path))\n",
        "predictions = evaluate(test_loader, model, device, calculate_accuracy=False)\n",
        "save_predictions(predictions, args.test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arAQwK7W0BHY",
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
